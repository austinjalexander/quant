{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(246987, 10) (246987, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>50dravg</th>\n",
       "      <th>200dravg</th>\n",
       "      <th>OC%</th>\n",
       "      <th>HL%</th>\n",
       "      <th>ticker</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>821</th>\n",
       "      <td>6.90</td>\n",
       "      <td>7.06</td>\n",
       "      <td>6.77</td>\n",
       "      <td>6.97</td>\n",
       "      <td>160500</td>\n",
       "      <td>8.5686</td>\n",
       "      <td>7.04795</td>\n",
       "      <td>0.010145</td>\n",
       "      <td>0.042836</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>822</th>\n",
       "      <td>7.06</td>\n",
       "      <td>7.60</td>\n",
       "      <td>7.03</td>\n",
       "      <td>7.33</td>\n",
       "      <td>175100</td>\n",
       "      <td>8.5522</td>\n",
       "      <td>7.04460</td>\n",
       "      <td>0.038244</td>\n",
       "      <td>0.081081</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>823</th>\n",
       "      <td>7.29</td>\n",
       "      <td>7.60</td>\n",
       "      <td>6.87</td>\n",
       "      <td>6.91</td>\n",
       "      <td>138100</td>\n",
       "      <td>8.5224</td>\n",
       "      <td>7.04130</td>\n",
       "      <td>-0.052126</td>\n",
       "      <td>0.106259</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>824</th>\n",
       "      <td>6.86</td>\n",
       "      <td>6.96</td>\n",
       "      <td>6.56</td>\n",
       "      <td>6.66</td>\n",
       "      <td>121400</td>\n",
       "      <td>8.4868</td>\n",
       "      <td>7.03935</td>\n",
       "      <td>-0.029155</td>\n",
       "      <td>0.060976</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>825</th>\n",
       "      <td>6.68</td>\n",
       "      <td>6.94</td>\n",
       "      <td>6.46</td>\n",
       "      <td>6.56</td>\n",
       "      <td>166700</td>\n",
       "      <td>8.4550</td>\n",
       "      <td>7.03830</td>\n",
       "      <td>-0.017964</td>\n",
       "      <td>0.074303</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Open  High   Low  Close  Volume  50dravg  200dravg       OC%       HL%  \\\n",
       "821  6.90  7.06  6.77   6.97  160500   8.5686   7.04795  0.010145  0.042836   \n",
       "822  7.06  7.60  7.03   7.33  175100   8.5522   7.04460  0.038244  0.081081   \n",
       "823  7.29  7.60  6.87   6.91  138100   8.5224   7.04130 -0.052126  0.106259   \n",
       "824  6.86  6.96  6.56   6.66  121400   8.4868   7.03935 -0.029155  0.060976   \n",
       "825  6.68  6.94  6.46   6.56  166700   8.4550   7.03830 -0.017964  0.074303   \n",
       "\n",
       "     ticker  \n",
       "821      78  \n",
       "822      78  \n",
       "823      78  \n",
       "824      78  \n",
       "825      78  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from module_imports import *\n",
    "from download_data import *\n",
    "from import_data import *\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "stock_df, prediction_df = pd.DataFrame(), pd.DataFrame()\n",
    "pred_tickers = []\n",
    "source = \"Q\"\n",
    "binarize = True\n",
    "gt = 0\n",
    "lt = 50.0\n",
    "vol = 0\n",
    "if source == \"Q\":\n",
    "    stock_df, prediction_df, pred_tickers = get_quandl_data(binarize=binarize, gt=gt, lt=lt, vol=vol)\n",
    "elif source == \"G\":\n",
    "    stock_df, prediction_df = get_goog_data(binarize=binarize, gt=gt, lt=lt, vol=vol)\n",
    "    \n",
    "Y = stock_df['label'].values\n",
    "Y = Y.reshape(Y.shape[0], 1)\n",
    "\n",
    "X_df = stock_df.drop('label', axis=1)\n",
    "X = X_df.values\n",
    "\n",
    "print X.shape, Y.shape\n",
    "X_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorize_label = True\n",
    "if vectorize_label == True:\n",
    "    new_y = []\n",
    "    positives = []\n",
    "    for i in xrange(Y.shape[0]):\n",
    "        if Y[i] == 0:\n",
    "            new_y.append(np.array([[1],[0]]))\n",
    "        elif Y[i] == 1:\n",
    "            new_y.append(np.array([[0],[1]]))\n",
    "    Y = new_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_vt, y_train, y_vt = train_test_split(X, Y, test_size=0.30, random_state=42)\n",
    "X_validation, X_test, y_validation, y_test = train_test_split(X_vt, y_vt, test_size=0.50, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.59493764]\n",
      " [-0.59308395]\n",
      " [-0.59384637]\n",
      " [-0.59969069]\n",
      " [-0.08063482]\n",
      " [-0.51664036]\n",
      " [-0.51954552]\n",
      " [-0.17759202]\n",
      " [ 0.05011843]\n",
      " [ 0.11802   ]]\n",
      "[[1]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "for x,y in zip(X_train, y_train):\n",
    "    print x.reshape(x.shape[0],1)\n",
    "    print y.reshape(y.shape[0],1)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "x = np.array([[0],\n",
    "              [1], \n",
    "              [2]])\n",
    "\n",
    "y = np.array([[0],\n",
    "              [1]])\n",
    "'''\n",
    "\n",
    "features = X_train[0].shape[0]\n",
    "h1 = 50\n",
    "h2 = 50\n",
    "outputs = y_train[0].shape[0]\n",
    "\n",
    "w1_init = np.sqrt(6)/np.sqrt(h1+features)\n",
    "W1 = np.random.uniform(low=-w1_init, high=w1_init, size=(h1*features)).reshape(h1,features)\n",
    "b1 = np.zeros((h1,1))\n",
    "#print \"W1\", W1.shape\n",
    "#print \"b1\", b1.shape\n",
    "\n",
    "w2_init = np.sqrt(6)/np.sqrt(h2+h1)\n",
    "W2 = np.random.uniform(low=-w2_init, high=w2_init, size=(h1*h2)).reshape(h1,h2)\n",
    "b2 = np.zeros((h2,1))\n",
    "#print \"W2\", W2.shape\n",
    "#print \"b2\", b2.shape\n",
    "\n",
    "w3_init = np.sqrt(6)/np.sqrt(outputs+h2)\n",
    "W3 = np.random.uniform(low=-w3_init, high=w3_init, size=(outputs*h2)).reshape(outputs,h2)\n",
    "b3 = np.zeros((outputs,1))\n",
    "#print \"W3\", W3.shape\n",
    "#print \"b3\", b3.shape\n",
    "\n",
    "#f_x = []\n",
    "\n",
    "print \"-\"*10\n",
    "\n",
    "def loss(f_x,y):\n",
    "    i = np.where(y == 1)[0][0]\n",
    "    return -np.log(f_x[i]) # negative log-likelihood\n",
    "\n",
    "def forward_prop(x, W1, b1, W2, b2, W3, b3):\n",
    "    def sigm(z):\n",
    "        return 1/(1+np.exp(-z))\n",
    "    \n",
    "    def softmax(z):\n",
    "        return np.exp(z)/np.sum(np.exp(z))\n",
    "\n",
    "    z1 = b1 + np.dot(W1,x)\n",
    "    a1 = sigm(z1)\n",
    "    \n",
    "    z2 = b2 + np.dot(W2,a1)\n",
    "    a2 = sigm(z2)\n",
    "    \n",
    "    z3 = b3 + np.dot(W3,a2)\n",
    "    a3 = softmax(z3)\n",
    "    \n",
    "    f_x = a3\n",
    "    return z1, a1, z2, a2, z3, a3, f_x\n",
    "'''\n",
    "z1, a1, z2, a2, z3, a3, f_x = forward_prop(x, W1, b1, W2, b2, W3, b3)\n",
    "print \"y\\n\", y\n",
    "print \"f_x\\n\", f_x\n",
    "print \"loss(f_x,y)\\n\", loss(f_x,y)\n",
    "print \"-\"*10\n",
    "'''\n",
    "\n",
    "def back_prop(x, W1, b1, W2, b2, W3, b3, z1, a1, z2, a2, z3, a3, f_x, y):\n",
    "    \n",
    "    def sigm(z):\n",
    "        return 1/(1+np.exp(-z))\n",
    "    \n",
    "    def sigm_prime(z):\n",
    "        return (sigm(z) * (1 - sigm(z)))\n",
    "    \n",
    "    del_z3 = -(y - f_x)\n",
    "    del_W3 = np.dot(del_z3,a2.T)\n",
    "    del_b3 = del_z3\n",
    "    \n",
    "    del_a2 = np.dot(W3.T,del_z3)\n",
    "    del_z2 = np.multiply(del_a2,sigm_prime(z2))\n",
    "    del_W2 = np.dot(del_z2,a1.T)\n",
    "    del_b2 = del_z2\n",
    "    \n",
    "    del_a1 = np.dot(W2.T,del_z2)\n",
    "    del_z1 = np.multiply(del_a1,sigm_prime(z1))\n",
    "    del_W1 = np.dot(del_z1,x.T)\n",
    "    del_b1 = del_z1\n",
    "    \n",
    "    return del_W1, del_b1, del_W2, del_b2, del_W3, del_b3\n",
    "#del_W1, del_b1, del_W2, del_b2, del_W3, del_b3 = back_prop(x, W1, b1, W2, b2, W3, b3, z1, a1, z2, a2, z3, a3, f_x, y)\n",
    "\n",
    "def finite_diff_approx(W, b, del_W, del_b, x, f_x, y):\n",
    "    \n",
    "    epsilon = 1e-6\n",
    "    \n",
    "    # W\n",
    "    approx_del_W = []\n",
    "    for i in xrange(W.shape[0]):\n",
    "        for j in xrange(W.shape[1]):\n",
    "\n",
    "            temp_w = (W[i][j])\n",
    "            W[i][j] = W[i][j]+epsilon\n",
    "            z1, a1, z2, a2, z3, a3, f_x = forward_prop(x, W, b1, W2, b2, W3, b3)\n",
    "            loss_left = loss(f_x,y)[0]\n",
    "            W[i][j] = temp_w\n",
    "\n",
    "            W[i][j] = (W[i][j]-epsilon)\n",
    "            z1, a1, z2, a2, z3, a3, f_x = forward_prop(x, W, b1, W2, b2, W3, b3)\n",
    "            loss_right = loss(f_x,y)[0]\n",
    "            W[i][j] = temp_w\n",
    "            \n",
    "            approx_del_W.append((loss_left - loss_right)/(2*epsilon))\n",
    "            \n",
    "    print \"\\nW gradient checking:\"\n",
    "    print \"\\tapprox_del_W\\n\\t\",approx_del_W[:3]\n",
    "    print \"\\tdel_W\\n\\t\",del_W.ravel()[:3]\n",
    "    print \"\\tapprox absolute difference:\", np.sum(np.abs(approx_del_W - del_W.ravel()))/(len(approx_del_W)**2)\n",
    "    \n",
    "    # b\n",
    "    approx_del_b = []\n",
    "    for i in xrange(b.shape[0]):\n",
    "        for j in xrange(b.shape[1]):\n",
    "\n",
    "            temp_b = b[i][j]\n",
    "            b[i][j] = (b[i][j]+epsilon)\n",
    "            z1, a1, z2, a2, z3, a3, f_x = forward_prop(x, W1, b, W2, b2, W3, b3)\n",
    "            loss_left = loss(f_x,y)[0]\n",
    "            b[i][j] = temp_b\n",
    "\n",
    "            b[i][j] = (b[i][j]-epsilon)\n",
    "            z1, a1, z2, a2, z3, a3, f_x = forward_prop(x, W1, b, W2, b2, W3, b3)\n",
    "            loss_right = loss(f_x,y)[0]\n",
    "            b[i][j] = temp_b\n",
    "            \n",
    "            approx_del_b.append((loss_left - loss_right)/(2*epsilon))\n",
    "            \n",
    "    print \"\\nb gradient checking:\"\n",
    "    print \"\\tapprox_del_b\\n\\t\",approx_del_b[:3]\n",
    "    print \"\\tdel_b\\n\\t\",del_b.ravel()[:3]\n",
    "    print \"\\tapprox absolute difference:\", np.sum(np.abs(approx_del_b - del_b.ravel()))/(len(approx_del_b)**2)\n",
    "\n",
    "#finite_diff_approx(W1, b1, del_W1, del_b1, x, f_x, y)\n",
    "\n",
    "def regularizer(LN, W):\n",
    "    if LN == 'L2':\n",
    "        # np.linalg.norm(W1)**2 + np.linalg.norm(W2)**2 + np.linalg.norm(W3)**2 # L2 regularization\n",
    "        return (2 * W) # W L2 gradient\n",
    "    elif LN == 'L1':\n",
    "        # np.sum(np.abs(W1)) + np.sum(np.abs(W2)) + np.sum(np.abs(W3)) # L1 regularization\n",
    "        return np.sign(W) # W L1 gradient\n",
    "    \n",
    "# SGD\n",
    "epochs = 10\n",
    "Lambda = 0.01\n",
    "LN = 'L2'\n",
    "alpha = 0.05\n",
    "#delta = 0.7 # 0.5 < delta <= 1\n",
    "training_losses = []\n",
    "validation_losses = []\n",
    "\n",
    "for i in xrange(epochs):\n",
    "    \n",
    "    # training\n",
    "    training_f_x = []\n",
    "    training_y = []\n",
    "    for x,y in zip(X_train, y_train):\n",
    "        \n",
    "        x = x.reshape(x.shape[0],1)\n",
    "        y = y.reshape(y.shape[0],1)\n",
    "\n",
    "        z1, a1, z2, a2, z3, a3, f_x = forward_prop(x, W1, b1, W2, b2, W3, b3)\n",
    "        del_W1, del_b1, del_W2, del_b2, del_W3, del_b3 = back_prop(x, W1, b1, W2, b2, W3, b3, z1, a1, z2, a2, z3, a3, f_x, y)\n",
    "        \n",
    "        deriv_W3 = -del_W3 - (Lambda * regularizer(LN, W3))\n",
    "        deriv_b3 = -del_b3\n",
    "        W3 = W3 + (alpha * deriv_W3)\n",
    "        b3 = b3 + (alpha * deriv_b3)\n",
    "\n",
    "        deriv_W2 = -del_W2 - (Lambda * regularizer(LN, W2))\n",
    "        deriv_b2 = -del_b2\n",
    "        W2 = W2 + (alpha * deriv_W2)\n",
    "        b2 = b2 + (alpha * deriv_b2)    \n",
    "\n",
    "        deriv_W1 = -del_W1 - (Lambda * regularizer(LN, W1))\n",
    "        deriv_b1 = -del_b1\n",
    "        W1 = W1 + (alpha * deriv_W1)\n",
    "        b1 = b1 + (alpha * deriv_b1)\n",
    "        \n",
    "        if np.isnan(W1[0])[0] == True:\n",
    "            raise ValueError('A very specific bad thing happened')\n",
    "            \n",
    "        training_f_x = f_x\n",
    "        training_y = y\n",
    "    training_losses.append(np.round(loss(training_f_x,training_y),2))\n",
    "            \n",
    "    # validation\n",
    "    validation_f_x = []\n",
    "    validation_y_v = []\n",
    "    for x,y in zip(X_validation, y_validation):\n",
    "\n",
    "        x = scaler.transform(x)\n",
    "        x = x.reshape(x.shape[0],1)\n",
    "        y = y.reshape(y.shape[0],1)\n",
    "\n",
    "        z1, a1, z2, a2, z3, a3, f_x = forward_prop(x, W1, b1, W2, b2, W3, b3)\n",
    "\n",
    "        if np.isnan(W1[0])[0] == True:\n",
    "            raise ValueError('A very specific bad thing happened')\n",
    "\n",
    "        validation_f_x = f_x\n",
    "        validation_y_v = y_v\n",
    "    validation_losses.append(np.round(loss(validation_f_x,validation_y_v),2))\n",
    "\n",
    "    print \"current training loss:\", training_losses[-1]\n",
    "    print \"current validation loss:\", validation_losses[-1]\n",
    "    plt.title('EPOCH ' + str(i))\n",
    "    plt.plot(training_losses)\n",
    "    plt.plot(validation_losses)\n",
    "    plt.legend(['Training','Validation'])\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.show()\n",
    "    \n",
    "    #if i > 5:\n",
    "    #    alpha = alpha/(1+(delta*i))\n",
    "    \n",
    "#z1, a1, z2, a2, z3, a3, f_x = forward_prop(x, W1, b1, W2, b2, W3, b3)\n",
    "#print \"y\\n\", y\n",
    "#print \"f_x\\n\", f_x\n",
    "#print \"loss(f_x,y)\\n\", loss(f_x,y)\n",
    "#print \"-\"*10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y\n",
      "[[0]\n",
      " [1]]\n",
      "f_x\n",
      "[[ nan]\n",
      " [ nan]]\n",
      "loss(f_x,y)\n",
      "[ nan]\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "i = 1400\n",
    "\n",
    "x_v = X_validation[i].reshape(X_validation[i].shape[0],1)\n",
    "y_v = y_validation[i].reshape(y_validation[i].shape[0],1)\n",
    "\n",
    "z1, a1, z2, a2, z3, a3, f_x = forward_prop(x_v, W1, b1, W2, b2, W3, b3)\n",
    "print \"y\\n\", y_v\n",
    "print \"f_x\\n\", f_x\n",
    "print \"loss(f_x,y)\\n\", loss(f_x,y_v)\n",
    "print \"-\"*10"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
