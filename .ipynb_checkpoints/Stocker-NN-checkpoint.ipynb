{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from module_imports import *\n",
    "from download_data import *\n",
    "from import_data import *\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#check_quandl_latest('HALO')\n",
    "#download_quandl()'\n",
    "#download_goog()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stock_df, prediction_df = pd.DataFrame(), pd.DataFrame()\n",
    "pred_tickers = []\n",
    "source = \"Q\"\n",
    "binarize = True\n",
    "gt = 2.0\n",
    "lt = 10.0\n",
    "vol = 1000\n",
    "if source == \"Q\":\n",
    "    stock_df, prediction_df, pred_tickers = get_quandl_data(binarize=True, gt=gt, lt=lt, vol=vol)\n",
    "elif source == \"G\":\n",
    "    stock_df, prediction_df = get_goog_data(binarize=True, gt=gt, lt=lt, vol=vol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(112991, 10) (112991, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>50dravg</th>\n",
       "      <th>200dravg</th>\n",
       "      <th>OC%</th>\n",
       "      <th>HL%</th>\n",
       "      <th>ticker</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>820</th>\n",
       "      <td>7.17</td>\n",
       "      <td>7.24</td>\n",
       "      <td>6.60</td>\n",
       "      <td>6.95</td>\n",
       "      <td>301600</td>\n",
       "      <td>8.5844</td>\n",
       "      <td>7.05000</td>\n",
       "      <td>-0.030683</td>\n",
       "      <td>0.096970</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>821</th>\n",
       "      <td>6.90</td>\n",
       "      <td>7.06</td>\n",
       "      <td>6.77</td>\n",
       "      <td>6.97</td>\n",
       "      <td>160500</td>\n",
       "      <td>8.5686</td>\n",
       "      <td>7.04795</td>\n",
       "      <td>0.010145</td>\n",
       "      <td>0.042836</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>822</th>\n",
       "      <td>7.06</td>\n",
       "      <td>7.60</td>\n",
       "      <td>7.03</td>\n",
       "      <td>7.33</td>\n",
       "      <td>175100</td>\n",
       "      <td>8.5522</td>\n",
       "      <td>7.04460</td>\n",
       "      <td>0.038244</td>\n",
       "      <td>0.081081</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>823</th>\n",
       "      <td>7.29</td>\n",
       "      <td>7.60</td>\n",
       "      <td>6.87</td>\n",
       "      <td>6.91</td>\n",
       "      <td>138100</td>\n",
       "      <td>8.5224</td>\n",
       "      <td>7.04130</td>\n",
       "      <td>-0.052126</td>\n",
       "      <td>0.106259</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>824</th>\n",
       "      <td>6.86</td>\n",
       "      <td>6.96</td>\n",
       "      <td>6.56</td>\n",
       "      <td>6.66</td>\n",
       "      <td>121400</td>\n",
       "      <td>8.4868</td>\n",
       "      <td>7.03935</td>\n",
       "      <td>-0.029155</td>\n",
       "      <td>0.060976</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Open  High   Low  Close  Volume  50dravg  200dravg       OC%       HL%  \\\n",
       "820  7.17  7.24  6.60   6.95  301600   8.5844   7.05000 -0.030683  0.096970   \n",
       "821  6.90  7.06  6.77   6.97  160500   8.5686   7.04795  0.010145  0.042836   \n",
       "822  7.06  7.60  7.03   7.33  175100   8.5522   7.04460  0.038244  0.081081   \n",
       "823  7.29  7.60  6.87   6.91  138100   8.5224   7.04130 -0.052126  0.106259   \n",
       "824  6.86  6.96  6.56   6.66  121400   8.4868   7.03935 -0.029155  0.060976   \n",
       "\n",
       "     ticker  \n",
       "820      78  \n",
       "821      78  \n",
       "822      78  \n",
       "823      78  \n",
       "824      78  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = stock_df['label'].values\n",
    "y = y.reshape(y.shape[0], 1)\n",
    "\n",
    "X_df = stock_df.drop('label', axis=1)\n",
    "X = X_df.values\n",
    "\n",
    "print X.shape, y.shape\n",
    "X_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#for col in X_df.columns:\n",
    "#    plt.title(col)\n",
    "#    plt.hist(X_df[col].values, bins=50, alpha=0.2)\n",
    "#    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    103258\n",
      "1      9733\n",
      "dtype: int64\n",
      "percentage positive: 0.09\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEKCAYAAAAMzhLIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJElF/wdxSqndsTZdiQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11040ccd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print stock_df['label'].value_counts()\n",
    "print \"percentage positive:\", np.round(np.true_divide(stock_df['label'].value_counts()[1], stock_df['label'].shape[0]), 2)\n",
    "plt.title('y values')\n",
    "plt.hist(y, bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorize_label = False\n",
    "if vectorize_label == True:\n",
    "    new_y = np.zeros((y.shape[0],2))\n",
    "    positives = []\n",
    "    for i in xrange(y.shape[0]):\n",
    "        if y[i] == 0:\n",
    "            new_y[i] = np.array([[1, 0]])\n",
    "        elif y[i] == 1:\n",
    "            new_y[i] = np.array([[0, 1]])\n",
    "            positives.append(i)\n",
    "\n",
    "    y = new_y\n",
    "    print y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "get_subset = False\n",
    "if get_subset == True:\n",
    "    indices = np.random.choice(X.shape[0], 10000)\n",
    "    X = X[indices,:]\n",
    "    y = y[indices, :]\n",
    "    print X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class NN(object):\n",
    "    \n",
    "    def __init__(self, num_nodes, Lambda=0):\n",
    "        \n",
    "        # hyperparameters\n",
    "        self.input_layer_size = num_nodes\n",
    "        self.hidden_layer_size = num_nodes\n",
    "        #self.hidden_layer1_size = 10\n",
    "        #self.hidden_layer2_size = 2\n",
    "        self.output_layer_size = 1\n",
    "        \n",
    "        # weights\n",
    "        ## for random samples from N(mu, sigma^2), use: sigma * np.random.randn(...) + mu\n",
    "        # (1/np.sqrt(10)) * --> Stack Overflow\n",
    "        # 4.59 * (1/np.sqrt(num_nodes+1)) * --> Yam/Chow\n",
    "        # (np.sqrt(6)/np.sqrt(self.input_layer_size + self.hidden_layer_size)) --> Bengio\n",
    "        W1_init_bound = (np.sqrt(6)/np.sqrt(self.input_layer_size + self.hidden_layer_size))\n",
    "        W1_total_values = self.input_layer_size * self.hidden_layer_size\n",
    "        W2_init_bound = (np.sqrt(6)/np.sqrt(self.hidden_layer_size + self.output_layer_size))\n",
    "        W2_total_values = self.hidden_layer_size * self.output_layer_size\n",
    "        \n",
    "        self.W1 = np.random.uniform(-W1_init_bound,W1_init_bound,W1_total_values).reshape(self.input_layer_size, self.hidden_layer_size)\n",
    "        self.W2 = np.random.uniform(-W2_init_bound,W2_init_bound,W2_total_values).reshape(self.hidden_layer_size, self.output_layer_size)\n",
    "        \n",
    "        #self.W1 = np.random.randn(self.input_layer_size, self.hidden_layer1_size)\n",
    "        #self.W2 = np.random.randn(self.hidden_layer1_size, self.hidden_layer2_size)\n",
    "        #self.W3 = np.random.randn(self.hidden_layer2_size, self.output_layer_size)\n",
    "        \n",
    "        # biases\n",
    "        self.b1 = 1.0\n",
    "        self.b2 = 1.0\n",
    "        \n",
    "        # regularization\n",
    "        self.Lambda = Lambda\n",
    "        \n",
    "    def get_weights(self):\n",
    "        return np.concatenate((self.W1.ravel(), self.W2.ravel()))\n",
    "        #return np.concatenate((self.W1.ravel(), self.W2.ravel(), self.W3.ravel()))\n",
    "    \n",
    "    def set_weights(self, weights):\n",
    "        W1_start  = 0\n",
    "        \n",
    "        W1_end = (self.hidden_layer_size * self.input_layer_size)\n",
    "        self.W1 = np.reshape(weights[W1_start:W1_end], (self.input_layer_size,self.hidden_layer_size))\n",
    "        #W1_end = (self.hidden_layer1_size * self.input_layer_size)\n",
    "        #self.W1 = np.reshape(weights[W1_start:W1_end], (self.input_layer_size,self.hidden_layer1_size))\n",
    "        \n",
    "        W2_end = W1_end + (self.hidden_layer_size * self.output_layer_size)\n",
    "        self.W2 = np.reshape(weights[W1_end:W2_end], (self.hidden_layer_size,self.output_layer_size))\n",
    "        \n",
    "        #W2_end = W1_end + (self.hidden_layer2_size * self.hidden_layer1_size)\n",
    "        #self.W2 = np.reshape(weights[W1_end:W2_end], (self.hidden_layer1_size,self.hidden_layer2_size))\n",
    "        #W3_end = W2_end + (self.hidden_layer2_size * self.output_layer_size)\n",
    "        #self.W3 = np.reshape(weights[W2_end:W3_end], (self.hidden_layer2_size,self.output_layer_size))\n",
    "        \n",
    "    def visualize_weights(self):\n",
    "        plt.title('Weight Distribution')\n",
    "        plt.legend(['W1','W2'])\n",
    "        plt.hist(self.W1.ravel(), bins=20, alpha=0.7)\n",
    "        plt.hist(self.W2.ravel(), bins=20, alpha=0.7)\n",
    "        plt.show()\n",
    "    \n",
    "    def forward_propagate(self, X):\n",
    "        self.z2 = np.dot(X, self.W1)\n",
    "        self.a2 = self.softplus(self.z2 + self.b1) \n",
    "        self.z3 = np.dot(self.a2, self.W2)\n",
    "        \n",
    "        y_hat = self.sigmoid(self.z3 + self.b2)\n",
    "        #y_hat = np.round(self.sigmoid(self.z3 + self.b2))\n",
    "        \n",
    "        #self.a3 = self.activation(self.z3) \n",
    "        #self.z4 = np.dot(self.a3, self.W3)    \n",
    "        #y_hat = self.activation(self.z4)\n",
    "        \n",
    "        return y_hat\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        return np.true_divide(1, (1 + np.exp(-z)))\n",
    "        \n",
    "    def sigmoid_prime(self, z):\n",
    "        return np.true_divide(np.exp(-z), ((1 + np.exp(-z))**2)) # sigmoid derivative\n",
    "    \n",
    "    def softplus(self, z): \n",
    "        return np.log(1 + np.exp(z)) # softplus (i.e., smooth rectified linear)\n",
    "    \n",
    "    def softplus_prime(self, z):\n",
    "        return np.true_divide(1, (1 + np.exp(-z))) # softplus derivative (i.e., sigmoid!)\n",
    "\n",
    "    def visualize_activation(self, func):\n",
    "        inputs = np.arange(-6,6,0.01)\n",
    "        plt.plot(inputs, func(inputs))\n",
    "        plt.show()\n",
    "        \n",
    "    def visualize_activation_prime(self, func, func_prime):\n",
    "        inputs = np.arange(-6,6,0.01)\n",
    "        plt.plot(inputs, func(inputs))\n",
    "        plt.plot(inputs, func_prime(inputs))\n",
    "        plt.show()\n",
    "        \n",
    "    def cost(self, X, y):\n",
    "        self.y_hat = self.forward_propagate(X)\n",
    "        left = 0.5 * np.sum((y - self.y_hat)**2)/X.shape[0]\n",
    "        right = (self.Lambda/2.0)*(np.sum(self.W1**2) + np.sum(self.W2**2))\n",
    "        \n",
    "        #right = (self.Lambda/2.0)*(np.sum(self.W1**2) + np.sum(self.W2**2) + np.sum(self.W3**2))\n",
    "        return left + right\n",
    "        \n",
    "    def cost_prime(self, X, y):\n",
    "        self.y_hat = self.forward_propagate(X)\n",
    "        \n",
    "        #delta4 =  np.multiply(-(y - self.y_hat), self.activation_prime(self.z4))\n",
    "        #dJdW3 = np.dot(self.a3.T, delta4)/X.shape[0] + (self.Lambda*self.W3)\n",
    "        \n",
    "        #delta3 =  np.dot(delta4, self.W3.T) * self.activation_prime(self.z3)\n",
    "        delta3 =  np.multiply(-(y - self.y_hat), self.sigmoid_prime(self.z3 + self.b2))\n",
    "        dJdW2 = np.dot(self.a2.T, delta3)/X.shape[0] + (self.Lambda*self.W2)\n",
    "        \n",
    "        delta2 = np.dot(delta3, self.W2.T) * self.softplus_prime(self.z2 + self.b1)\n",
    "        dJdW1 = np.dot(X.T, delta2)/X.shape[0] + (self.Lambda*self.W1)\n",
    "        \n",
    "        #return dJdW1, dJdW2, dJdW3\n",
    "        return dJdW1, dJdW2\n",
    "                           \n",
    "    def compute_gradient(self, X, y):\n",
    "        #dJdW1, dJdW2, dJdW3 = self.cost_prime(X, y)\n",
    "        dJdW1, dJdW2 = self.cost_prime(X, y)\n",
    "        #return np.concatenate((dJdW1.ravel(), dJdW2.ravel(), dJdW3.ravel()))\n",
    "        return np.concatenate((dJdW1.ravel(), dJdW2.ravel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_weights():\n",
    "    nn = NN()\n",
    "    nn.visualize_weights()\n",
    "#test_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_activation():\n",
    "    nn = NN(num_nodes=np.arange(-6,6,0.01).shape[0], Lambda=0.001)\n",
    "    #nn.visualize_activation_prime(nn.sigmoid, nn.sigmoid_prime)\n",
    "    #nn.visualize_activation_prime(nn.softplus, nn.softplus_prime)\n",
    "#test_activation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def testNN():\n",
    "    nn = NN(num_nodes=3, Lambda=0.01)\n",
    "    y_hat = nn.forward_propagate(X)\n",
    "    print y_hat, \"\\n\"\n",
    "    print y, \"\\n\"\n",
    "    \n",
    "    cost1 = nn.cost(X,y)\n",
    "    print cost1, \"\\n\"\n",
    "\n",
    "    dJdW1, dJdW2 = nn.cost_prime(X,y)\n",
    "    print dJdW1, \"\\n\"\n",
    "    print dJdW2\n",
    "#testNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def estimate_gradient(nn_test, nn_test_X, nn_test_y):\n",
    "    weights = nn_test.get_weights()\n",
    "    estimated_gradient = np.zeros(weights.shape)\n",
    "    perturb = np.zeros(weights.shape)\n",
    "    epsilon = 1e-4\n",
    "    \n",
    "    for i in xrange(len(weights)):\n",
    "        perturb[i] = epsilon\n",
    "        \n",
    "        nn_test.set_weights(weights + perturb)\n",
    "        loss2 = nn_test.cost(nn_test_X, nn_test_y)\n",
    "        \n",
    "        nn_test.set_weights(weights - perturb)\n",
    "        loss1 = nn_test.cost(nn_test_X, nn_test_y)\n",
    "        \n",
    "        estimated_gradient[i] = (loss2 - loss1) / (2 * epsilon)\n",
    "        \n",
    "        perturb[i] = 0\n",
    "    \n",
    "    nn_test.set_weights(weights)\n",
    "        \n",
    "    return estimated_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_gradient_estimation(nn_test, nn_test_X, nn_test_y):\n",
    "    \n",
    "    #nn_test_X = StandardScaler().fit_transform(nn_test_X)\n",
    "    #nn_test_X = np.hstack((np.ones((nn_test_X.shape[0], 1)), nn_test_X))\n",
    "\n",
    "    estimated_gradient = estimate_gradient(nn_test, nn_test_X, nn_test_y)\n",
    "    gradient = nn_test.compute_gradient(nn_test_X, nn_test_y)\n",
    "\n",
    "    print \"\\n\\t--- Gradient Checking ---\"\n",
    "    print estimated_gradient[:3]\n",
    "    print gradient[:3], \"\\n\"\n",
    "    print \"\\tdiff:\", np.linalg.norm(gradient-estimated_gradient)/np.linalg.norm(gradient+estimated_gradient)   \n",
    "    print \"\\t-------------------------\"\n",
    "    \n",
    "test_grad_est = False\n",
    "if test_grad_est == True:\n",
    "    test_gradient_estimation(nn_test, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class trainer(object):\n",
    "    \n",
    "    def __init__(self, N):\n",
    "        self.N = N\n",
    "        \n",
    "    def callback(self, weights):\n",
    "        self.N.set_weights(weights)\n",
    "        self.J.append(self.N.cost(self.X_train, self.y_train))\n",
    "        self.test_J.append(self.N.cost(self.X_test, self.y_test))\n",
    "        \n",
    "    def cost_wrapper(self, weights, X, y):\n",
    "        self.N.set_weights(weights)\n",
    "        c = self.N.cost(X, y)\n",
    "        g = self.N.compute_gradient(X,y)\n",
    "        \n",
    "        return c, g\n",
    "    \n",
    "    def set_scale(self, X):\n",
    "        self.scaler = StandardScaler().fit(X)\n",
    "        \n",
    "    def get_scale(self):\n",
    "        return self.scaler\n",
    "    \n",
    "    def add_bias(self, X):\n",
    "        return np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "    \n",
    "    def trainBFGS(self, X, y):\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "        \n",
    "        self.set_scale(X_train)\n",
    "        scaler = self.get_scale()\n",
    "        \n",
    "        self.X_train = scaler.transform(X_train)\n",
    "        #self.X_train = self.add_bias(self.X_train)\n",
    "        self.y_train = y_train # StandardScaler().fit_transform(y_train)\n",
    "        \n",
    "        self.X_test = scaler.transform(X_test)\n",
    "        #self.X_test = self.add_bias(self.X_test)\n",
    "        self.y_test = y_test # StandardScaler().fit_transform(y_test)\n",
    "        \n",
    "        self.J = []\n",
    "        self.test_J = []\n",
    "        \n",
    "        weights0 = self.N.get_weights()\n",
    "        \n",
    "        options = {'maxiter':500, 'disp':True}\n",
    "        _res = optimize.minimize(self.cost_wrapper, weights0, jac=True, method='BFGS', args=(self.X_train,self.y_train), options=options, callback=self.callback)\n",
    "        \n",
    "        self.N.set_weights(_res.x)\n",
    "        self.iterations = _res\n",
    "        self.optimization_results = _res\n",
    "        \n",
    "    def trainGD(self, X, y):\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        \n",
    "        self.set_scale(X_train)\n",
    "        scaler = self.get_scale()\n",
    "        \n",
    "        self.X_train = scaler.transform(X_train)\n",
    "        #self.X_train = self.add_bias(self.X_train)\n",
    "        self.y_train = y_train # StandardScaler().fit_transform(y_train)\n",
    "        \n",
    "        self.X_test = scaler.transform(X_test)\n",
    "        #self.X_test = self.add_bias(self.X_test)\n",
    "        self.y_test = y_test # StandardScaler().fit_transform(y_test)\n",
    "        \n",
    "        self.J = []\n",
    "        self.test_J = []\n",
    "        \n",
    "        self.iterations = 1000\n",
    "        self.alpha = 0.01\n",
    "        for iteration in xrange(self.iterations):\n",
    "            dJdW1, dJdW2 = self.N.cost_prime(self.X_train, self.y_train)\n",
    "            self.W1 = self.alpha * dJdW1\n",
    "            self.W2 = self.alpha * dJdW2\n",
    "            self.J.append(self.N.cost(self.X_train, self.y_train))\n",
    "            self.test_J.append(self.N.cost(self.X_test, self.y_test))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "time for training: 155.47 seconds\n",
      "\n",
      "confusion matrix:\n",
      "    F   T\n",
      "F [    0 20599]\n",
      "T  [   0 2000]\n",
      "\n",
      "classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00     20599\n",
      "          1       0.09      1.00      0.16      2000\n",
      "\n",
      "avg / total       0.01      0.09      0.01     22599\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAECCAYAAADtg+DiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1104c5590>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEKCAYAAAACS67iAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1100cf110>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "time0 = time()\n",
    "\n",
    "nn = NN(num_nodes=X.shape[1], Lambda=0.1) # 0.0001\n",
    "Trainer = trainer(nn)\n",
    "train_alg = 'GD'\n",
    "if train_alg == 'BFGS':\n",
    "    Trainer.trainBFGS(X,y)\n",
    "elif train_alg == 'GD':\n",
    "    Trainer.trainGD(X,y)\n",
    "\n",
    "#batches = 10\n",
    "#for batch in xrange(batches):\n",
    "#    indices = np.random.choice(X.shape[0], 10000)\n",
    "#    X1 = X[indices,:]\n",
    "#    y1 = y[indices, :]\n",
    "#    Trainer.trainBFGS(X1,y1)\n",
    "\n",
    "#test_grad_est = True\n",
    "#if test_grad_est == True:\n",
    "#    test_gradient_estimation(nn, Trainer.X_test, Trainer.y_test)\n",
    "\n",
    "train_time = np.round((time() - time0),2)\n",
    "print \"\\ntime for training:\", train_time, \"seconds\"\n",
    "\n",
    "binary_output = True\n",
    "pos_precision = 0\n",
    "if binary_output == True:\n",
    "    y_test = np.round(Trainer.y_test)\n",
    "    y_pred = np.round(nn.forward_propagate(Trainer.X_test))\n",
    "    \n",
    "    print \"\\nconfusion matrix:\"\n",
    "    print \"    F   T\"\n",
    "    print \"F\", confusion_matrix(y_test, y_pred)[0]\n",
    "    print \"T \", confusion_matrix(y_test, y_pred)[1]\n",
    "    print \"\\nclassification report:\"\n",
    "    print classification_report(y_test, y_pred)\n",
    "    pos_precision = float(classification_report(y_test, y_pred).split()[-11])\n",
    "\n",
    "plt.plot(Trainer.J)\n",
    "plt.plot(Trainer.test_J)\n",
    "plt.legend(['J', 'test_J'])\n",
    "plt.show()\n",
    "\n",
    "nn.visualize_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0 ABIO [[ 0.99]]\n",
      " 1 ACOR [[ 0.99]]\n",
      " 2 ADMA [[ 0.99]]\n",
      " 3 AERI [[ 0.99]]\n",
      " 4 AFFX [[ 0.99]]\n",
      " 5 AGEN [[ 0.99]]\n",
      " 6 APPY [[ 0.99]]\n",
      " 7 ARDM [[ 1.]]\n",
      " 8 ARIA [[ 1.]]\n",
      " 9 ARNA [[ 1.]]\n",
      "10 ARWR [[ 1.]]\n",
      "11 AXDX [[ 0.99]]\n",
      "12 AXGN [[ 1.]]\n",
      "13 BABY [[ 1.]]\n",
      "14 BASI [[ 1.]]\n",
      "15 BCLI [[ 0.99]]\n",
      "16 BCRX [[ 0.99]]\n",
      "17 BGMD [[ 0.98]]\n",
      "18 BIIB [[ 0.99]]\n",
      "19 BLUE [[ 0.99]]\n",
      "20 BOTA [[ 0.98]]\n",
      "21 BRKR [[ 0.98]]\n",
      "22 CBLI [[ 0.99]]\n",
      "23 CBMG [[ 0.98]]\n",
      "24 CBMX [[ 0.98]]\n",
      "25 CBPO [[ 0.98]]\n",
      "26 CGEN [[ 0.99]]\n",
      "27 CLDN [[ 0.97]]\n",
      "28 CLDX [[ 0.99]]\n",
      "29 CNMD [[ 0.96]]\n",
      "30 COHR [[ 0.99]]\n",
      "31 CPHD [[ 0.98]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXUAAAEKCAYAAADticXcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEENdkgpiqEtSQf4/3ByYFN4SrfsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10fccc750>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred_df = prediction_df[prediction_df['label'].apply(np.isnan) == True]\n",
    "pred_X = pred_df.drop(['label'], axis=1).values\n",
    "predictions = []\n",
    "high_prob_pred_count = 0\n",
    "\n",
    "for i in xrange(pred_X.shape[0]):\n",
    "    scaler = Trainer.get_scale()\n",
    "    x = scaler.transform(pred_X[i:i+1,0:pred_X.shape[1]])\n",
    "    #x = Trainer.add_bias(x)\n",
    "    y_hat = nn.forward_propagate(x)\n",
    "    if np.round(y_hat,2) >= 0.8:\n",
    "        print str(i).rjust(2), str(pred_tickers[i]).rjust(4), np.round(y_hat,2)\n",
    "        high_prob_pred_count += 1\n",
    "    predictions.append(y_hat[0][0])\n",
    "    \n",
    "plt.title('Prediction Distribution')\n",
    "plt.hist(predictions, bins=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>binarize</th>\n",
       "      <th>gt</th>\n",
       "      <th>lt</th>\n",
       "      <th>vol</th>\n",
       "      <th>Trainer.N.input_layer_size</th>\n",
       "      <th>Trainer.N.hidden_layer_size</th>\n",
       "      <th>Trainer.N.output_layer_size</th>\n",
       "      <th>Trainer.N.b1</th>\n",
       "      <th>Trainer.N.b2</th>\n",
       "      <th>Trainer.N.Lambda</th>\n",
       "      <th>train_alg</th>\n",
       "      <th>Trainer.iterations</th>\n",
       "      <th>Trainer.alpha</th>\n",
       "      <th>train_time</th>\n",
       "      <th>Trainer.J</th>\n",
       "      <th>Trainer.test_J</th>\n",
       "      <th>pos_precision</th>\n",
       "      <th>high_prob_pred_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Q</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>100</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>150.0</td>\n",
       "      <td>GD</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.10</td>\n",
       "      <td>212.24</td>\n",
       "      <td>'NaN'</td>\n",
       "      <td>'NaN'</td>\n",
       "      <td>0.08</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Q</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>100</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>GD</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>211.06</td>\n",
       "      <td>'NaN'</td>\n",
       "      <td>'NaN'</td>\n",
       "      <td>0.08</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Q</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>GD</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>165.15</td>\n",
       "      <td>'NaN'</td>\n",
       "      <td>'NaN'</td>\n",
       "      <td>0.09</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Q</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>GD</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>163.31</td>\n",
       "      <td>0.981454585791</td>\n",
       "      <td>0.980923207252</td>\n",
       "      <td>0.09</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Q</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>10000</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>GD</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>125.42</td>\n",
       "      <td>0.581854858764</td>\n",
       "      <td>0.582359736298</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  source binarize  gt  lt    vol  Trainer.N.input_layer_size  \\\n",
       "2      Q     True   2  20    100                          10   \n",
       "3      Q     True   2  20    100                          10   \n",
       "4      Q     True   2  10    100                          10   \n",
       "5      Q     True   2  10    100                          10   \n",
       "6      Q     True   2  10  10000                          10   \n",
       "\n",
       "   Trainer.N.hidden_layer_size  Trainer.N.output_layer_size  Trainer.N.b1  \\\n",
       "2                           10                            1             1   \n",
       "3                           10                            1             1   \n",
       "4                           10                            1             1   \n",
       "5                           10                            1             1   \n",
       "6                           10                            1             1   \n",
       "\n",
       "   Trainer.N.b2  Trainer.N.Lambda train_alg  Trainer.iterations  \\\n",
       "2             1             150.0        GD                1000   \n",
       "3             1               0.1        GD                1000   \n",
       "4             1               0.1        GD                1000   \n",
       "5             1               0.1        GD                1000   \n",
       "6             1               0.1        GD                1000   \n",
       "\n",
       "   Trainer.alpha  train_time       Trainer.J  Trainer.test_J  pos_precision  \\\n",
       "2           0.10      212.24           'NaN'           'NaN'           0.08   \n",
       "3           0.01      211.06           'NaN'           'NaN'           0.08   \n",
       "4           0.01      165.15           'NaN'           'NaN'           0.09   \n",
       "5           0.01      163.31  0.981454585791  0.980923207252           0.09   \n",
       "6           0.01      125.42  0.581854858764  0.582359736298           0.00   \n",
       "\n",
       "   high_prob_pred_count  \n",
       "2                    43  \n",
       "3                    43  \n",
       "4                    14  \n",
       "5                    32  \n",
       "6                     0  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report_df = pd.read_csv('nn_report.csv')\n",
    "report_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>binarize</th>\n",
       "      <th>gt</th>\n",
       "      <th>lt</th>\n",
       "      <th>vol</th>\n",
       "      <th>Trainer.N.input_layer_size</th>\n",
       "      <th>Trainer.N.hidden_layer_size</th>\n",
       "      <th>Trainer.N.output_layer_size</th>\n",
       "      <th>Trainer.N.b1</th>\n",
       "      <th>Trainer.N.b2</th>\n",
       "      <th>Trainer.N.Lambda</th>\n",
       "      <th>train_alg</th>\n",
       "      <th>Trainer.iterations</th>\n",
       "      <th>Trainer.alpha</th>\n",
       "      <th>train_time</th>\n",
       "      <th>Trainer.J</th>\n",
       "      <th>Trainer.test_J</th>\n",
       "      <th>pos_precision</th>\n",
       "      <th>high_prob_pred_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Q</td>\n",
       "      <td>True</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1000</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>GD</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>155.47</td>\n",
       "      <td>0.955871330113</td>\n",
       "      <td>0.954511536057</td>\n",
       "      <td>0.09</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  source binarize   gt    lt   vol Trainer.N.input_layer_size  \\\n",
       "0      Q     True  2.0  10.0  1000                         10   \n",
       "\n",
       "  Trainer.N.hidden_layer_size Trainer.N.output_layer_size Trainer.N.b1  \\\n",
       "0                          10                           1          1.0   \n",
       "\n",
       "  Trainer.N.b2 Trainer.N.Lambda train_alg Trainer.iterations Trainer.alpha  \\\n",
       "0          1.0              0.1        GD               1000          0.01   \n",
       "\n",
       "  train_time       Trainer.J  Trainer.test_J pos_precision  \\\n",
       "0     155.47  0.955871330113  0.954511536057          0.09   \n",
       "\n",
       "  high_prob_pred_count  \n",
       "0                   32  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_to_record = [source,binarize,gt,lt,vol,Trainer.N.input_layer_size,Trainer.N.hidden_layer_size,Trainer.N.output_layer_size,Trainer.N.b1,Trainer.N.b2,Trainer.N.Lambda,train_alg,Trainer.iterations,Trainer.alpha,train_time,Trainer.J[-1],Trainer.test_J[-1],pos_precision,high_prob_pred_count]\n",
    "data_to_record = np.array(data_to_record).reshape(1,len(data_to_record))\n",
    "data_df = pd.DataFrame(data_to_record, columns=report_df.columns)\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_report_df = report_df.append(data_df)\n",
    "new_report_df.to_csv('nn_report.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report_df = pd.read_csv('nn_report.csv')\n",
    "True in report_df.duplicated(['source','binarize','gt','lt','vol','Trainer.N.input_layer_size','Trainer.N.hidden_layer_size','Trainer.N.output_layer_size','Trainer.N.b1','Trainer.N.b2','Trainer.N.Lambda','train_alg','Trainer.alpha']).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>binarize</th>\n",
       "      <th>gt</th>\n",
       "      <th>lt</th>\n",
       "      <th>vol</th>\n",
       "      <th>Trainer.N.input_layer_size</th>\n",
       "      <th>Trainer.N.hidden_layer_size</th>\n",
       "      <th>Trainer.N.output_layer_size</th>\n",
       "      <th>Trainer.N.b1</th>\n",
       "      <th>Trainer.N.b2</th>\n",
       "      <th>Trainer.N.Lambda</th>\n",
       "      <th>train_alg</th>\n",
       "      <th>Trainer.iterations</th>\n",
       "      <th>Trainer.alpha</th>\n",
       "      <th>train_time</th>\n",
       "      <th>Trainer.J</th>\n",
       "      <th>Trainer.test_J</th>\n",
       "      <th>pos_precision</th>\n",
       "      <th>high_prob_pred_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Q</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>100</td>\n",
       "      <td>9</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>150.0</td>\n",
       "      <td>GD</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.10</td>\n",
       "      <td>343.62</td>\n",
       "      <td>'NaN'</td>\n",
       "      <td>'NaN'</td>\n",
       "      <td>0.09</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Q</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>100</td>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>150.0</td>\n",
       "      <td>GD</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.10</td>\n",
       "      <td>377.26</td>\n",
       "      <td>'NaN'</td>\n",
       "      <td>'NaN'</td>\n",
       "      <td>0.09</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Q</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>100</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>150.0</td>\n",
       "      <td>GD</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.10</td>\n",
       "      <td>212.24</td>\n",
       "      <td>'NaN'</td>\n",
       "      <td>'NaN'</td>\n",
       "      <td>0.08</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Q</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>100</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>GD</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>211.06</td>\n",
       "      <td>'NaN'</td>\n",
       "      <td>'NaN'</td>\n",
       "      <td>0.08</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Q</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>GD</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>165.15</td>\n",
       "      <td>'NaN'</td>\n",
       "      <td>'NaN'</td>\n",
       "      <td>0.09</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Q</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>GD</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>163.31</td>\n",
       "      <td>0.981454585791</td>\n",
       "      <td>0.980923207252</td>\n",
       "      <td>0.09</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Q</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>10000</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>GD</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>125.42</td>\n",
       "      <td>0.581854858764</td>\n",
       "      <td>0.582359736298</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Q</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>1000</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>GD</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>155.47</td>\n",
       "      <td>0.955871330113</td>\n",
       "      <td>0.954511536057</td>\n",
       "      <td>0.09</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  source binarize  gt  lt    vol  Trainer.N.input_layer_size  \\\n",
       "0      Q     True   2  20    100                           9   \n",
       "1      Q     True   2  20    100                          10   \n",
       "2      Q     True   2  20    100                          10   \n",
       "3      Q     True   2  20    100                          10   \n",
       "4      Q     True   2  10    100                          10   \n",
       "5      Q     True   2  10    100                          10   \n",
       "6      Q     True   2  10  10000                          10   \n",
       "7      Q     True   2  10   1000                          10   \n",
       "\n",
       "   Trainer.N.hidden_layer_size  Trainer.N.output_layer_size  Trainer.N.b1  \\\n",
       "0                           18                            1             1   \n",
       "1                           20                            1             1   \n",
       "2                           10                            1             1   \n",
       "3                           10                            1             1   \n",
       "4                           10                            1             1   \n",
       "5                           10                            1             1   \n",
       "6                           10                            1             1   \n",
       "7                           10                            1             1   \n",
       "\n",
       "   Trainer.N.b2  Trainer.N.Lambda train_alg  Trainer.iterations  \\\n",
       "0             1             150.0        GD                1000   \n",
       "1             1             150.0        GD                1000   \n",
       "2             1             150.0        GD                1000   \n",
       "3             1               0.1        GD                1000   \n",
       "4             1               0.1        GD                1000   \n",
       "5             1               0.1        GD                1000   \n",
       "6             1               0.1        GD                1000   \n",
       "7             1               0.1        GD                1000   \n",
       "\n",
       "   Trainer.alpha  train_time       Trainer.J  Trainer.test_J  pos_precision  \\\n",
       "0           0.10      343.62           'NaN'           'NaN'           0.09   \n",
       "1           0.10      377.26           'NaN'           'NaN'           0.09   \n",
       "2           0.10      212.24           'NaN'           'NaN'           0.08   \n",
       "3           0.01      211.06           'NaN'           'NaN'           0.08   \n",
       "4           0.01      165.15           'NaN'           'NaN'           0.09   \n",
       "5           0.01      163.31  0.981454585791  0.980923207252           0.09   \n",
       "6           0.01      125.42  0.581854858764  0.582359736298           0.00   \n",
       "7           0.01      155.47  0.955871330113  0.954511536057           0.09   \n",
       "\n",
       "   high_prob_pred_count  \n",
       "0                    26  \n",
       "1                    17  \n",
       "2                    43  \n",
       "3                    43  \n",
       "4                    14  \n",
       "5                    32  \n",
       "6                     0  \n",
       "7                    32  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
