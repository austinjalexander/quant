{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from module_imports import *\n",
    "from download_data import *\n",
    "from import_data import *\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "problem with IMRS\n"
     ]
    }
   ],
   "source": [
    "#check_quandl_latest('HALO')\n",
    "#download_quandl()\n",
    "download_goog()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stock_df, prediction_df = pd.DataFrame(), pd.DataFrame()\n",
    "pred_tickers = []\n",
    "source = \"Q\"\n",
    "binarize = True\n",
    "gt = 0\n",
    "lt = 50.0\n",
    "vol = 0\n",
    "if source == \"Q\":\n",
    "    stock_df, prediction_df, pred_tickers = get_quandl_data(binarize=True, gt=gt, lt=lt, vol=vol)\n",
    "elif source == \"G\":\n",
    "    stock_df, prediction_df = get_goog_data(binarize=True, gt=gt, lt=lt, vol=vol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(246987, 10) (246987, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>50dravg</th>\n",
       "      <th>200dravg</th>\n",
       "      <th>OC%</th>\n",
       "      <th>HL%</th>\n",
       "      <th>ticker</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>821</th>\n",
       "      <td>6.90</td>\n",
       "      <td>7.06</td>\n",
       "      <td>6.77</td>\n",
       "      <td>6.97</td>\n",
       "      <td>160500</td>\n",
       "      <td>8.5686</td>\n",
       "      <td>7.04795</td>\n",
       "      <td>0.010145</td>\n",
       "      <td>0.042836</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>822</th>\n",
       "      <td>7.06</td>\n",
       "      <td>7.60</td>\n",
       "      <td>7.03</td>\n",
       "      <td>7.33</td>\n",
       "      <td>175100</td>\n",
       "      <td>8.5522</td>\n",
       "      <td>7.04460</td>\n",
       "      <td>0.038244</td>\n",
       "      <td>0.081081</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>823</th>\n",
       "      <td>7.29</td>\n",
       "      <td>7.60</td>\n",
       "      <td>6.87</td>\n",
       "      <td>6.91</td>\n",
       "      <td>138100</td>\n",
       "      <td>8.5224</td>\n",
       "      <td>7.04130</td>\n",
       "      <td>-0.052126</td>\n",
       "      <td>0.106259</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>824</th>\n",
       "      <td>6.86</td>\n",
       "      <td>6.96</td>\n",
       "      <td>6.56</td>\n",
       "      <td>6.66</td>\n",
       "      <td>121400</td>\n",
       "      <td>8.4868</td>\n",
       "      <td>7.03935</td>\n",
       "      <td>-0.029155</td>\n",
       "      <td>0.060976</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>825</th>\n",
       "      <td>6.68</td>\n",
       "      <td>6.94</td>\n",
       "      <td>6.46</td>\n",
       "      <td>6.56</td>\n",
       "      <td>166700</td>\n",
       "      <td>8.4550</td>\n",
       "      <td>7.03830</td>\n",
       "      <td>-0.017964</td>\n",
       "      <td>0.074303</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Open  High   Low  Close  Volume  50dravg  200dravg       OC%       HL%  \\\n",
       "821  6.90  7.06  6.77   6.97  160500   8.5686   7.04795  0.010145  0.042836   \n",
       "822  7.06  7.60  7.03   7.33  175100   8.5522   7.04460  0.038244  0.081081   \n",
       "823  7.29  7.60  6.87   6.91  138100   8.5224   7.04130 -0.052126  0.106259   \n",
       "824  6.86  6.96  6.56   6.66  121400   8.4868   7.03935 -0.029155  0.060976   \n",
       "825  6.68  6.94  6.46   6.56  166700   8.4550   7.03830 -0.017964  0.074303   \n",
       "\n",
       "     ticker  \n",
       "821      78  \n",
       "822      78  \n",
       "823      78  \n",
       "824      78  \n",
       "825      78  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = stock_df['label'].values\n",
    "y = y.reshape(y.shape[0], 1)\n",
    "\n",
    "X_df = stock_df.drop('label', axis=1)\n",
    "X = X_df.values\n",
    "\n",
    "print X.shape, y.shape\n",
    "X_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#for col in X_df.columns:\n",
    "#    plt.title(col)\n",
    "#    plt.hist(X_df[col].values, bins=50, alpha=0.2)\n",
    "#    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    225234\n",
      "1     21753\n",
      "dtype: int64\n",
      "percentage positive: 0.09\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEKCAYAAADjDHn2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nigwLSVKRYSFJKvp/eZJ9e5BuuBoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1094348d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print stock_df['label'].value_counts()\n",
    "print \"percentage positive:\", np.round(np.true_divide(stock_df['label'].value_counts()[1], stock_df['label'].shape[0]), 2)\n",
    "plt.title('y values')\n",
    "plt.hist(y, bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorize_label = False\n",
    "if vectorize_label == True:\n",
    "    new_y = np.zeros((y.shape[0],2))\n",
    "    positives = []\n",
    "    for i in xrange(y.shape[0]):\n",
    "        if y[i] == 0:\n",
    "            new_y[i] = np.array([[1, 0]])\n",
    "        elif y[i] == 1:\n",
    "            new_y[i] = np.array([[0, 1]])\n",
    "            positives.append(i)\n",
    "\n",
    "    y = new_y\n",
    "    print y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "get_subset = False\n",
    "if get_subset == True:\n",
    "    indices = np.random.choice(X.shape[0], 10000)\n",
    "    X = X[indices,:]\n",
    "    y = y[indices, :]\n",
    "    print X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class NN(object):\n",
    "    \n",
    "    def __init__(self, num_nodes, Lambda=0):\n",
    "        \n",
    "        # hyperparameters\n",
    "        self.input_layer_size = num_nodes\n",
    "        self.hidden_layer_size = num_nodes\n",
    "        #self.hidden_layer1_size = 10\n",
    "        #self.hidden_layer2_size = 2\n",
    "        self.output_layer_size = 1\n",
    "        \n",
    "        # weights\n",
    "        ## for random samples from N(mu, sigma^2), use: sigma * np.random.randn(...) + mu\n",
    "        # (1/np.sqrt(10)) * --> Stack Overflow\n",
    "        # 4.59 * (1/np.sqrt(num_nodes+1)) * --> Yam/Chow\n",
    "        # (np.sqrt(6)/np.sqrt(self.input_layer_size + self.hidden_layer_size)) --> Bengio\n",
    "        W1_init_bound = (np.sqrt(6)/np.sqrt(self.input_layer_size + self.hidden_layer_size))\n",
    "        W1_total_values = self.input_layer_size * self.hidden_layer_size\n",
    "        W2_init_bound = (np.sqrt(6)/np.sqrt(self.hidden_layer_size + self.output_layer_size))\n",
    "        W2_total_values = self.hidden_layer_size * self.output_layer_size\n",
    "        \n",
    "        self.W1 = np.random.uniform(-W1_init_bound,W1_init_bound,W1_total_values).reshape(self.input_layer_size, self.hidden_layer_size)\n",
    "        self.W2 = np.random.uniform(-W2_init_bound,W2_init_bound,W2_total_values).reshape(self.hidden_layer_size, self.output_layer_size)\n",
    "        \n",
    "        #self.W1 = np.random.randn(self.input_layer_size, self.hidden_layer1_size)\n",
    "        #self.W2 = np.random.randn(self.hidden_layer1_size, self.hidden_layer2_size)\n",
    "        #self.W3 = np.random.randn(self.hidden_layer2_size, self.output_layer_size)\n",
    "        \n",
    "        # biases\n",
    "        self.b1 = 1.0\n",
    "        self.b2 = 1.0\n",
    "        \n",
    "        # regularization\n",
    "        self.Lambda = Lambda\n",
    "        \n",
    "    def get_weights(self):\n",
    "        return np.concatenate((self.W1.ravel(), self.W2.ravel()))\n",
    "        #return np.concatenate((self.W1.ravel(), self.W2.ravel(), self.W3.ravel()))\n",
    "    \n",
    "    def set_weights(self, weights):\n",
    "        W1_start  = 0\n",
    "        \n",
    "        W1_end = (self.hidden_layer_size * self.input_layer_size)\n",
    "        self.W1 = np.reshape(weights[W1_start:W1_end], (self.input_layer_size,self.hidden_layer_size))\n",
    "        #W1_end = (self.hidden_layer1_size * self.input_layer_size)\n",
    "        #self.W1 = np.reshape(weights[W1_start:W1_end], (self.input_layer_size,self.hidden_layer1_size))\n",
    "        \n",
    "        W2_end = W1_end + (self.hidden_layer_size * self.output_layer_size)\n",
    "        self.W2 = np.reshape(weights[W1_end:W2_end], (self.hidden_layer_size,self.output_layer_size))\n",
    "        \n",
    "        #W2_end = W1_end + (self.hidden_layer2_size * self.hidden_layer1_size)\n",
    "        #self.W2 = np.reshape(weights[W1_end:W2_end], (self.hidden_layer1_size,self.hidden_layer2_size))\n",
    "        #W3_end = W2_end + (self.hidden_layer2_size * self.output_layer_size)\n",
    "        #self.W3 = np.reshape(weights[W2_end:W3_end], (self.hidden_layer2_size,self.output_layer_size))\n",
    "        \n",
    "    def visualize_weights(self):\n",
    "        plt.title('Weight Distribution')\n",
    "        plt.legend(['W1','W2'])\n",
    "        plt.hist(self.W1.ravel(), bins=20, alpha=0.7)\n",
    "        plt.hist(self.W2.ravel(), bins=20, alpha=0.7)\n",
    "        plt.show()\n",
    "    \n",
    "    def forward_propagate(self, X):\n",
    "        self.z2 = np.dot(X, self.W1)\n",
    "        self.a2 = self.softplus(self.z2 + self.b1) \n",
    "        self.z3 = np.dot(self.a2, self.W2)\n",
    "        \n",
    "        y_hat = self.sigmoid(self.z3 + self.b2)\n",
    "        #y_hat = np.round(self.sigmoid(self.z3 + self.b2))\n",
    "        \n",
    "        #self.a3 = self.activation(self.z3) \n",
    "        #self.z4 = np.dot(self.a3, self.W3)    \n",
    "        #y_hat = self.activation(self.z4)\n",
    "        \n",
    "        return y_hat\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        return np.true_divide(1, (1 + np.exp(-z)))\n",
    "        \n",
    "    def sigmoid_prime(self, z):\n",
    "        return np.true_divide(np.exp(-z), ((1 + np.exp(-z))**2)) # sigmoid derivative\n",
    "    \n",
    "    def softplus(self, z): \n",
    "        return np.log(1 + np.exp(z)) # softplus (i.e., smooth rectified linear)\n",
    "    \n",
    "    def softplus_prime(self, z):\n",
    "        return np.true_divide(1, (1 + np.exp(-z))) # softplus derivative (i.e., sigmoid!)\n",
    "\n",
    "    def visualize_activation(self, func):\n",
    "        inputs = np.arange(-6,6,0.01)\n",
    "        plt.plot(inputs, func(inputs))\n",
    "        plt.show()\n",
    "        \n",
    "    def visualize_activation_prime(self, func, func_prime):\n",
    "        inputs = np.arange(-6,6,0.01)\n",
    "        plt.plot(inputs, func(inputs))\n",
    "        plt.plot(inputs, func_prime(inputs))\n",
    "        plt.show()\n",
    "        \n",
    "    def cost(self, X, y):\n",
    "        self.y_hat = self.forward_propagate(X)\n",
    "        left = 0.5 * np.sum((y - self.y_hat)**2)/X.shape[0]\n",
    "        right = (self.Lambda/2.0)*(np.sum(self.W1**2) + np.sum(self.W2**2))\n",
    "        \n",
    "        #right = (self.Lambda/2.0)*(np.sum(self.W1**2) + np.sum(self.W2**2) + np.sum(self.W3**2))\n",
    "        return left + right\n",
    "        \n",
    "    def cost_prime(self, X, y):\n",
    "        self.y_hat = self.forward_propagate(X)\n",
    "        \n",
    "        #delta4 =  np.multiply(-(y - self.y_hat), self.activation_prime(self.z4))\n",
    "        #dJdW3 = np.dot(self.a3.T, delta4)/X.shape[0] + (self.Lambda*self.W3)\n",
    "        \n",
    "        #delta3 =  np.dot(delta4, self.W3.T) * self.activation_prime(self.z3)\n",
    "        delta3 =  np.multiply(-(y - self.y_hat), self.sigmoid_prime(self.z3 + self.b2))\n",
    "        dJdW2 = np.dot(self.a2.T, delta3)/X.shape[0] + (self.Lambda*self.W2)\n",
    "        \n",
    "        delta2 = np.dot(delta3, self.W2.T) * self.softplus_prime(self.z2 + self.b1)\n",
    "        dJdW1 = np.dot(X.T, delta2)/X.shape[0] + (self.Lambda*self.W1)\n",
    "        \n",
    "        #return dJdW1, dJdW2, dJdW3\n",
    "        return dJdW1, dJdW2\n",
    "                           \n",
    "    def compute_gradient(self, X, y):\n",
    "        #dJdW1, dJdW2, dJdW3 = self.cost_prime(X, y)\n",
    "        dJdW1, dJdW2 = self.cost_prime(X, y)\n",
    "        #return np.concatenate((dJdW1.ravel(), dJdW2.ravel(), dJdW3.ravel()))\n",
    "        return np.concatenate((dJdW1.ravel(), dJdW2.ravel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_weights():\n",
    "    nn = NN()\n",
    "    nn.visualize_weights()\n",
    "#test_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_activation():\n",
    "    nn = NN(num_nodes=np.arange(-6,6,0.01).shape[0], Lambda=0.001)\n",
    "    #nn.visualize_activation_prime(nn.sigmoid, nn.sigmoid_prime)\n",
    "    #nn.visualize_activation_prime(nn.softplus, nn.softplus_prime)\n",
    "#test_activation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def testNN():\n",
    "    nn = NN(num_nodes=3, Lambda=0.01)\n",
    "    y_hat = nn.forward_propagate(X)\n",
    "    print y_hat, \"\\n\"\n",
    "    print y, \"\\n\"\n",
    "    \n",
    "    cost1 = nn.cost(X,y)\n",
    "    print cost1, \"\\n\"\n",
    "\n",
    "    dJdW1, dJdW2 = nn.cost_prime(X,y)\n",
    "    print dJdW1, \"\\n\"\n",
    "    print dJdW2\n",
    "#testNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def estimate_gradient(nn_test, nn_test_X, nn_test_y):\n",
    "    weights = nn_test.get_weights()\n",
    "    estimated_gradient = np.zeros(weights.shape)\n",
    "    perturb = np.zeros(weights.shape)\n",
    "    epsilon = 1e-4\n",
    "    \n",
    "    for i in xrange(len(weights)):\n",
    "        perturb[i] = epsilon\n",
    "        \n",
    "        nn_test.set_weights(weights + perturb)\n",
    "        loss2 = nn_test.cost(nn_test_X, nn_test_y)\n",
    "        \n",
    "        nn_test.set_weights(weights - perturb)\n",
    "        loss1 = nn_test.cost(nn_test_X, nn_test_y)\n",
    "        \n",
    "        estimated_gradient[i] = (loss2 - loss1) / (2 * epsilon)\n",
    "        \n",
    "        perturb[i] = 0\n",
    "    \n",
    "    nn_test.set_weights(weights)\n",
    "        \n",
    "    return estimated_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_gradient_estimation(nn_test, nn_test_X, nn_test_y):\n",
    "    \n",
    "    #nn_test_X = StandardScaler().fit_transform(nn_test_X)\n",
    "    #nn_test_X = np.hstack((np.ones((nn_test_X.shape[0], 1)), nn_test_X))\n",
    "\n",
    "    estimated_gradient = estimate_gradient(nn_test, nn_test_X, nn_test_y)\n",
    "    gradient = nn_test.compute_gradient(nn_test_X, nn_test_y)\n",
    "\n",
    "    print \"\\n\\t--- Gradient Checking ---\"\n",
    "    print estimated_gradient[:3]\n",
    "    print gradient[:3], \"\\n\"\n",
    "    print \"\\tdiff:\", np.linalg.norm(gradient-estimated_gradient)/np.linalg.norm(gradient+estimated_gradient)   \n",
    "    print \"\\t-------------------------\"\n",
    "    \n",
    "test_grad_est = False\n",
    "if test_grad_est == True:\n",
    "    test_gradient_estimation(nn_test, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class trainer(object):\n",
    "    \n",
    "    def __init__(self, N):\n",
    "        self.N = N\n",
    "        \n",
    "    def callback(self, weights):\n",
    "        self.N.set_weights(weights)\n",
    "        self.J.append(self.N.cost(self.X_train, self.y_train))\n",
    "        self.test_J.append(self.N.cost(self.X_test, self.y_test))\n",
    "        \n",
    "    def cost_wrapper(self, weights, X, y):\n",
    "        self.N.set_weights(weights)\n",
    "        c = self.N.cost(X, y)\n",
    "        g = self.N.compute_gradient(X,y)\n",
    "        \n",
    "        return c, g\n",
    "    \n",
    "    def set_scale(self, X):\n",
    "        self.scaler = StandardScaler().fit(X)\n",
    "        \n",
    "    def get_scale(self):\n",
    "        return self.scaler\n",
    "    \n",
    "    def add_bias(self, X):\n",
    "        return np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "    \n",
    "    def trainBFGS(self, X, y):\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "        \n",
    "        self.set_scale(X_train)\n",
    "        scaler = self.get_scale()\n",
    "        \n",
    "        self.X_train = scaler.transform(X_train)\n",
    "        #self.X_train = self.add_bias(self.X_train)\n",
    "        self.y_train = y_train # StandardScaler().fit_transform(y_train)\n",
    "        \n",
    "        self.X_test = scaler.transform(X_test)\n",
    "        #self.X_test = self.add_bias(self.X_test)\n",
    "        self.y_test = y_test # StandardScaler().fit_transform(y_test)\n",
    "        \n",
    "        self.J = []\n",
    "        self.test_J = []\n",
    "        \n",
    "        weights0 = self.N.get_weights()\n",
    "        \n",
    "        options = {'maxiter':500, 'disp':True}\n",
    "        _res = optimize.minimize(self.cost_wrapper, weights0, jac=True, method='BFGS', args=(self.X_train,self.y_train), options=options, callback=self.callback)\n",
    "        \n",
    "        self.N.set_weights(_res.x)\n",
    "        self.iterations = _res\n",
    "        self.optimization_results = _res\n",
    "        \n",
    "    def trainGD(self, X, y):\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        \n",
    "        self.set_scale(X_train)\n",
    "        scaler = self.get_scale()\n",
    "        \n",
    "        self.X_train = scaler.transform(X_train)\n",
    "        #self.X_train = self.add_bias(self.X_train)\n",
    "        self.y_train = y_train # StandardScaler().fit_transform(y_train)\n",
    "        \n",
    "        self.X_test = scaler.transform(X_test)\n",
    "        #self.X_test = self.add_bias(self.X_test)\n",
    "        self.y_test = y_test # StandardScaler().fit_transform(y_test)\n",
    "        \n",
    "        self.J = []\n",
    "        self.test_J = []\n",
    "        \n",
    "        self.iterations = 1000\n",
    "        self.alpha = 0.01\n",
    "        for iteration in xrange(self.iterations):\n",
    "            dJdW1, dJdW2 = self.N.cost_prime(self.X_train, self.y_train)\n",
    "            self.W1 = self.alpha * dJdW1\n",
    "            self.W2 = self.alpha * dJdW2\n",
    "            self.J.append(self.N.cost(self.X_train, self.y_train))\n",
    "            self.test_J.append(self.N.cost(self.X_test, self.y_test))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "time for training: 317.87 seconds\n",
      "\n",
      "confusion matrix:\n",
      "    F   T\n",
      "F [    0 45041]\n",
      "T  [   0 4357]\n",
      "\n",
      "classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00     45041\n",
      "          1       0.09      1.00      0.16      4357\n",
      "\n",
      "avg / total       0.01      0.09      0.01     49398\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/sklearn/metrics/classification.py:958: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEHCAYAAACEKcAKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10635ec90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEKCAYAAAACS67iAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10a49dc90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "time0 = time()\n",
    "\n",
    "nn = NN(num_nodes=X.shape[1], Lambda=0.1) # 0.0001\n",
    "Trainer = trainer(nn)\n",
    "train_alg = 'GD'\n",
    "if train_alg == 'BFGS':\n",
    "    Trainer.trainBFGS(X,y)\n",
    "elif train_alg == 'GD':\n",
    "    Trainer.trainGD(X,y)\n",
    "\n",
    "#batches = 10\n",
    "#for batch in xrange(batches):\n",
    "#    indices = np.random.choice(X.shape[0], 10000)\n",
    "#    X1 = X[indices,:]\n",
    "#    y1 = y[indices, :]\n",
    "#    Trainer.trainBFGS(X1,y1)\n",
    "\n",
    "#test_grad_est = True\n",
    "#if test_grad_est == True:\n",
    "#    test_gradient_estimation(nn, Trainer.X_test, Trainer.y_test)\n",
    "\n",
    "train_time = np.round((time() - time0),2)\n",
    "print \"\\ntime for training:\", train_time, \"seconds\"\n",
    "\n",
    "binary_output = True\n",
    "pos_precision = 0\n",
    "if binary_output == True:\n",
    "    y_test = np.round(Trainer.y_test)\n",
    "    y_pred = np.round(nn.forward_propagate(Trainer.X_test))\n",
    "    \n",
    "    print \"\\nconfusion matrix:\"\n",
    "    print \"    F   T\"\n",
    "    print \"F\", confusion_matrix(y_test, y_pred)[0]\n",
    "    print \"T \", confusion_matrix(y_test, y_pred)[1]\n",
    "    print \"\\nclassification report:\"\n",
    "    print classification_report(y_test, y_pred)\n",
    "    pos_precision = float(classification_report(y_test, y_pred).split()[-11])\n",
    "\n",
    "plt.plot(Trainer.J)\n",
    "plt.plot(Trainer.test_J)\n",
    "plt.legend(['J', 'test_J'])\n",
    "plt.show()\n",
    "\n",
    "nn.visualize_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0 ABIO [[ 0.85]]\n",
      " 1 ACOR [[ 0.93]]\n",
      " 2 ADMA [[ 0.85]]\n",
      " 3 AERI [[ 0.92]]\n",
      " 4 AFFX [[ 0.86]]\n",
      " 5 AGEN [[ 0.89]]\n",
      " 6 APPY [[ 0.86]]\n",
      " 7 ARDM [[ 0.86]]\n",
      " 8 ARIA [[ 0.89]]\n",
      " 9 ARNA [[ 0.89]]\n",
      "10 ARWR [[ 0.91]]\n",
      "11 AXDX [[ 0.89]]\n",
      "12 AXGN [[ 0.84]]\n",
      "13 BABY [[ 0.97]]\n",
      "14 BASI [[ 0.83]]\n",
      "15 BCLI [[ 0.9]]\n",
      "16 BCRX [[ 0.87]]\n",
      "17 BGMD [[ 0.89]]\n",
      "18 BIIB [[ 0.87]]\n",
      "19 BLUE [[ 0.91]]\n",
      "20 BOTA [[ 0.86]]\n",
      "21 BRKR [[ 0.93]]\n",
      "22 CBLI [[ 0.85]]\n",
      "23 CBMG [[ 0.9]]\n",
      "24 CBMX [[ 0.91]]\n",
      "25 CBPO [[ 0.92]]\n",
      "26 CGEN [[ 0.99]]\n",
      "27 CLDN [[ 0.9]]\n",
      "28 CLDX [[ 0.9]]\n",
      "29 CNMD [[ 0.91]]\n",
      "30 COHR [[ 0.96]]\n",
      "31 CPHD [[ 0.89]]\n",
      "32 CPRX [[ 0.88]]\n",
      "33 CRIS [[ 0.91]]\n",
      "34 CUTR [[ 0.93]]\n",
      "35 CYBX [[ 0.94]]\n",
      "36 CYNO [[ 0.91]]\n",
      "37 CYTR [[ 0.9]]\n",
      "38 DARA [[ 0.9]]\n",
      "39 DRAD [[ 0.92]]\n",
      "40 DSCO [[ 0.92]]\n",
      "41 DYAX [[ 0.94]]\n",
      "42 ECTE [[ 0.92]]\n",
      "43 ECYT [[ 0.92]]\n",
      "44 ELOS [[ 0.95]]\n",
      "45 ENZN [[ 0.92]]\n",
      "46 ESMC [[ 0.92]]\n",
      "47 ETRM [[ 0.93]]\n",
      "48 EXAS [[ 0.97]]\n",
      "49 EXEL [[ 0.91]]\n",
      "50 FATE [[ 0.9]]\n",
      "51 FEIC [[ 0.93]]\n",
      "52 FLDM [[ 0.99]]\n",
      "53 FONR [[ 0.94]]\n",
      "54 GEVA [[ 0.92]]\n",
      "55 GILD [[ 0.94]]\n",
      "56 GNCA [[ 0.93]]\n",
      "57 HALO [[ 0.93]]\n",
      "58 HSKA [[ 0.94]]\n",
      "59 IART [[ 0.94]]\n",
      "60 ICCC [[ 0.92]]\n",
      "61 IDRA [[ 0.93]]\n",
      "62 IDXX [[ 0.92]]\n",
      "63 ILMN [[ 0.93]]\n",
      "64 IMMU [[ 0.94]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAEKCAYAAAAGvn7fAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\npOgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x109ebd1d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred_df = prediction_df[prediction_df['label'].apply(np.isnan) == True]\n",
    "pred_X = pred_df.drop(['label'], axis=1).values\n",
    "predictions = []\n",
    "high_prob_pred_count = 0\n",
    "\n",
    "for i in xrange(pred_X.shape[0]):\n",
    "    scaler = Trainer.get_scale()\n",
    "    x = scaler.transform(pred_X[i:i+1,0:pred_X.shape[1]])\n",
    "    #x = Trainer.add_bias(x)\n",
    "    y_hat = nn.forward_propagate(x)\n",
    "    if np.round(y_hat,2) >= 0.5:\n",
    "        print str(i).rjust(2), str(pred_tickers[i]).rjust(4), np.round(y_hat,2)\n",
    "        high_prob_pred_count += 1\n",
    "    predictions.append(y_hat[0][0])\n",
    "    \n",
    "plt.title('Prediction Distribution')\n",
    "plt.hist(predictions, bins=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>binarize</th>\n",
       "      <th>gt</th>\n",
       "      <th>lt</th>\n",
       "      <th>vol</th>\n",
       "      <th>Trainer.N.input_layer_size</th>\n",
       "      <th>Trainer.N.hidden_layer_size</th>\n",
       "      <th>Trainer.N.output_layer_size</th>\n",
       "      <th>Trainer.N.b1</th>\n",
       "      <th>Trainer.N.b2</th>\n",
       "      <th>Trainer.N.Lambda</th>\n",
       "      <th>train_alg</th>\n",
       "      <th>Trainer.iterations</th>\n",
       "      <th>Trainer.alpha</th>\n",
       "      <th>train_time</th>\n",
       "      <th>Trainer.J</th>\n",
       "      <th>Trainer.test_J</th>\n",
       "      <th>pos_precision</th>\n",
       "      <th>high_prob_pred_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Q</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>1000</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10.00</td>\n",
       "      <td>GD</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>148.20</td>\n",
       "      <td>56.723945047</td>\n",
       "      <td>56.7242597141</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Q</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>1000</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>100.00</td>\n",
       "      <td>GD</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>152.29</td>\n",
       "      <td>618.262421314</td>\n",
       "      <td>618.262270192</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Q</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>1000</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>GD</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>153.60</td>\n",
       "      <td>0.221258104669</td>\n",
       "      <td>0.220241662672</td>\n",
       "      <td>0.09</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Q</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>GD</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>270.86</td>\n",
       "      <td>0.469649346741</td>\n",
       "      <td>0.46790881602</td>\n",
       "      <td>0.10</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Q</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.10</td>\n",
       "      <td>GD</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>273.11</td>\n",
       "      <td>0.725289119473</td>\n",
       "      <td>0.725394965164</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   source binarize  gt  lt   vol  Trainer.N.input_layer_size  \\\n",
       "9       Q     True   2  10  1000                          10   \n",
       "10      Q     True   2  10  1000                          10   \n",
       "11      Q     True   2  10  1000                          10   \n",
       "12      Q     True   0  20     0                          10   \n",
       "13      Q     True   0  20     0                          10   \n",
       "\n",
       "    Trainer.N.hidden_layer_size  Trainer.N.output_layer_size  Trainer.N.b1  \\\n",
       "9                            10                            1             1   \n",
       "10                           10                            1             1   \n",
       "11                           10                            1             1   \n",
       "12                           10                            1             1   \n",
       "13                           10                            1             1   \n",
       "\n",
       "    Trainer.N.b2  Trainer.N.Lambda train_alg  Trainer.iterations  \\\n",
       "9              1             10.00        GD                1000   \n",
       "10             1            100.00        GD                1000   \n",
       "11             1              0.01        GD                1000   \n",
       "12             1              0.01        GD                1000   \n",
       "13             1              0.10        GD                1000   \n",
       "\n",
       "    Trainer.alpha  train_time       Trainer.J  Trainer.test_J  pos_precision  \\\n",
       "9            0.01      148.20    56.723945047   56.7242597141           0.10   \n",
       "10           0.01      152.29   618.262421314   618.262270192           0.09   \n",
       "11           0.01      153.60  0.221258104669  0.220241662672           0.09   \n",
       "12           0.01      270.86  0.469649346741   0.46790881602           0.10   \n",
       "13           0.01      273.11  0.725289119473  0.725394965164           0.11   \n",
       "\n",
       "    high_prob_pred_count  \n",
       "9                      0  \n",
       "10                     0  \n",
       "11                     7  \n",
       "12                    58  \n",
       "13                     0  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report_df = pd.read_csv('nn_report.csv')\n",
    "report_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>binarize</th>\n",
       "      <th>gt</th>\n",
       "      <th>lt</th>\n",
       "      <th>vol</th>\n",
       "      <th>Trainer.N.input_layer_size</th>\n",
       "      <th>Trainer.N.hidden_layer_size</th>\n",
       "      <th>Trainer.N.output_layer_size</th>\n",
       "      <th>Trainer.N.b1</th>\n",
       "      <th>Trainer.N.b2</th>\n",
       "      <th>Trainer.N.Lambda</th>\n",
       "      <th>train_alg</th>\n",
       "      <th>Trainer.iterations</th>\n",
       "      <th>Trainer.alpha</th>\n",
       "      <th>train_time</th>\n",
       "      <th>Trainer.J</th>\n",
       "      <th>Trainer.test_J</th>\n",
       "      <th>pos_precision</th>\n",
       "      <th>high_prob_pred_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Q</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>GD</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>317.87</td>\n",
       "      <td>0.961476743753</td>\n",
       "      <td>0.961692399746</td>\n",
       "      <td>0.09</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  source binarize gt    lt vol Trainer.N.input_layer_size  \\\n",
       "0      Q     True  0  50.0   0                         10   \n",
       "\n",
       "  Trainer.N.hidden_layer_size Trainer.N.output_layer_size Trainer.N.b1  \\\n",
       "0                          10                           1          1.0   \n",
       "\n",
       "  Trainer.N.b2 Trainer.N.Lambda train_alg Trainer.iterations Trainer.alpha  \\\n",
       "0          1.0              0.1        GD               1000          0.01   \n",
       "\n",
       "  train_time       Trainer.J  Trainer.test_J pos_precision  \\\n",
       "0     317.87  0.961476743753  0.961692399746          0.09   \n",
       "\n",
       "  high_prob_pred_count  \n",
       "0                   65  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_to_record = [source,binarize,gt,lt,vol,Trainer.N.input_layer_size,Trainer.N.hidden_layer_size,Trainer.N.output_layer_size,Trainer.N.b1,Trainer.N.b2,Trainer.N.Lambda,train_alg,Trainer.iterations,Trainer.alpha,train_time,Trainer.J[-1],Trainer.test_J[-1],pos_precision,high_prob_pred_count]\n",
    "data_to_record = np.array(data_to_record).reshape(1,len(data_to_record))\n",
    "data_df = pd.DataFrame(data_to_record, columns=report_df.columns)\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_report_df = report_df.append(data_df)\n",
    "new_report_df.to_csv('nn_report.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report_df = pd.read_csv('nn_report.csv')\n",
    "True in report_df.duplicated(['source','binarize','gt','lt','vol','Trainer.N.input_layer_size','Trainer.N.hidden_layer_size','Trainer.N.output_layer_size','Trainer.N.b1','Trainer.N.b2','Trainer.N.Lambda','train_alg','Trainer.alpha']).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>binarize</th>\n",
       "      <th>gt</th>\n",
       "      <th>lt</th>\n",
       "      <th>vol</th>\n",
       "      <th>Trainer.N.input_layer_size</th>\n",
       "      <th>Trainer.N.hidden_layer_size</th>\n",
       "      <th>Trainer.N.output_layer_size</th>\n",
       "      <th>Trainer.N.b1</th>\n",
       "      <th>Trainer.N.b2</th>\n",
       "      <th>Trainer.N.Lambda</th>\n",
       "      <th>train_alg</th>\n",
       "      <th>Trainer.iterations</th>\n",
       "      <th>Trainer.alpha</th>\n",
       "      <th>train_time</th>\n",
       "      <th>Trainer.J</th>\n",
       "      <th>Trainer.test_J</th>\n",
       "      <th>pos_precision</th>\n",
       "      <th>high_prob_pred_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Q</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>100</td>\n",
       "      <td>9</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>150.00</td>\n",
       "      <td>GD</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.10</td>\n",
       "      <td>343.62</td>\n",
       "      <td>'NaN'</td>\n",
       "      <td>'NaN'</td>\n",
       "      <td>0.09</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Q</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>100</td>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>150.00</td>\n",
       "      <td>GD</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.10</td>\n",
       "      <td>377.26</td>\n",
       "      <td>'NaN'</td>\n",
       "      <td>'NaN'</td>\n",
       "      <td>0.09</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Q</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>100</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>150.00</td>\n",
       "      <td>GD</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.10</td>\n",
       "      <td>212.24</td>\n",
       "      <td>'NaN'</td>\n",
       "      <td>'NaN'</td>\n",
       "      <td>0.08</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Q</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>100</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.10</td>\n",
       "      <td>GD</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>211.06</td>\n",
       "      <td>'NaN'</td>\n",
       "      <td>'NaN'</td>\n",
       "      <td>0.08</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Q</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.10</td>\n",
       "      <td>GD</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>165.15</td>\n",
       "      <td>'NaN'</td>\n",
       "      <td>'NaN'</td>\n",
       "      <td>0.09</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Q</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.10</td>\n",
       "      <td>GD</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>163.31</td>\n",
       "      <td>0.981454585791</td>\n",
       "      <td>0.980923207252</td>\n",
       "      <td>0.09</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Q</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>10000</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.10</td>\n",
       "      <td>GD</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>125.42</td>\n",
       "      <td>0.581854858764</td>\n",
       "      <td>0.582359736298</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Q</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>1000</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.10</td>\n",
       "      <td>GD</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>155.47</td>\n",
       "      <td>0.955871330113</td>\n",
       "      <td>0.954511536057</td>\n",
       "      <td>0.09</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Q</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>1000</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.10</td>\n",
       "      <td>GD</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>150.15</td>\n",
       "      <td>0.862765589364</td>\n",
       "      <td>0.86419577884</td>\n",
       "      <td>0.09</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Q</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>1000</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10.00</td>\n",
       "      <td>GD</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>148.20</td>\n",
       "      <td>56.723945047</td>\n",
       "      <td>56.7242597141</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Q</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>1000</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>100.00</td>\n",
       "      <td>GD</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>152.29</td>\n",
       "      <td>618.262421314</td>\n",
       "      <td>618.262270192</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Q</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>1000</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>GD</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>153.60</td>\n",
       "      <td>0.221258104669</td>\n",
       "      <td>0.220241662672</td>\n",
       "      <td>0.09</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Q</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>GD</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>270.86</td>\n",
       "      <td>0.469649346741</td>\n",
       "      <td>0.46790881602</td>\n",
       "      <td>0.10</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Q</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.10</td>\n",
       "      <td>GD</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>273.11</td>\n",
       "      <td>0.725289119473</td>\n",
       "      <td>0.725394965164</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Q</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.10</td>\n",
       "      <td>GD</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>317.87</td>\n",
       "      <td>0.961476743753</td>\n",
       "      <td>0.961692399746</td>\n",
       "      <td>0.09</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   source binarize  gt  lt    vol  Trainer.N.input_layer_size  \\\n",
       "0       Q     True   2  20    100                           9   \n",
       "1       Q     True   2  20    100                          10   \n",
       "2       Q     True   2  20    100                          10   \n",
       "3       Q     True   2  20    100                          10   \n",
       "4       Q     True   2  10    100                          10   \n",
       "5       Q     True   2  10    100                          10   \n",
       "6       Q     True   2  10  10000                          10   \n",
       "7       Q     True   2  10   1000                          10   \n",
       "8       Q     True   2  10   1000                          10   \n",
       "9       Q     True   2  10   1000                          10   \n",
       "10      Q     True   2  10   1000                          10   \n",
       "11      Q     True   2  10   1000                          10   \n",
       "12      Q     True   0  20      0                          10   \n",
       "13      Q     True   0  20      0                          10   \n",
       "14      Q     True   0  50      0                          10   \n",
       "\n",
       "    Trainer.N.hidden_layer_size  Trainer.N.output_layer_size  Trainer.N.b1  \\\n",
       "0                            18                            1             1   \n",
       "1                            20                            1             1   \n",
       "2                            10                            1             1   \n",
       "3                            10                            1             1   \n",
       "4                            10                            1             1   \n",
       "5                            10                            1             1   \n",
       "6                            10                            1             1   \n",
       "7                            10                            1             1   \n",
       "8                            10                            1             1   \n",
       "9                            10                            1             1   \n",
       "10                           10                            1             1   \n",
       "11                           10                            1             1   \n",
       "12                           10                            1             1   \n",
       "13                           10                            1             1   \n",
       "14                           10                            1             1   \n",
       "\n",
       "    Trainer.N.b2  Trainer.N.Lambda train_alg  Trainer.iterations  \\\n",
       "0              1            150.00        GD                1000   \n",
       "1              1            150.00        GD                1000   \n",
       "2              1            150.00        GD                1000   \n",
       "3              1              0.10        GD                1000   \n",
       "4              1              0.10        GD                1000   \n",
       "5              1              0.10        GD                1000   \n",
       "6              1              0.10        GD                1000   \n",
       "7              1              0.10        GD                1000   \n",
       "8              1              0.10        GD                1000   \n",
       "9              1             10.00        GD                1000   \n",
       "10             1            100.00        GD                1000   \n",
       "11             1              0.01        GD                1000   \n",
       "12             1              0.01        GD                1000   \n",
       "13             1              0.10        GD                1000   \n",
       "14             1              0.10        GD                1000   \n",
       "\n",
       "    Trainer.alpha  train_time       Trainer.J  Trainer.test_J  pos_precision  \\\n",
       "0            0.10      343.62           'NaN'           'NaN'           0.09   \n",
       "1            0.10      377.26           'NaN'           'NaN'           0.09   \n",
       "2            0.10      212.24           'NaN'           'NaN'           0.08   \n",
       "3            0.01      211.06           'NaN'           'NaN'           0.08   \n",
       "4            0.01      165.15           'NaN'           'NaN'           0.09   \n",
       "5            0.01      163.31  0.981454585791  0.980923207252           0.09   \n",
       "6            0.01      125.42  0.581854858764  0.582359736298           0.00   \n",
       "7            0.01      155.47  0.955871330113  0.954511536057           0.09   \n",
       "8            0.01      150.15  0.862765589364   0.86419577884           0.09   \n",
       "9            0.01      148.20    56.723945047   56.7242597141           0.10   \n",
       "10           0.01      152.29   618.262421314   618.262270192           0.09   \n",
       "11           0.01      153.60  0.221258104669  0.220241662672           0.09   \n",
       "12           0.01      270.86  0.469649346741   0.46790881602           0.10   \n",
       "13           0.01      273.11  0.725289119473  0.725394965164           0.11   \n",
       "14           0.01      317.87  0.961476743753  0.961692399746           0.09   \n",
       "\n",
       "    high_prob_pred_count  \n",
       "0                     26  \n",
       "1                     17  \n",
       "2                     43  \n",
       "3                     43  \n",
       "4                     14  \n",
       "5                     32  \n",
       "6                      0  \n",
       "7                     32  \n",
       "8                     16  \n",
       "9                      0  \n",
       "10                     0  \n",
       "11                     7  \n",
       "12                    58  \n",
       "13                     0  \n",
       "14                    65  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
